The repository runs quantized gguf models locally using llama cpp 

# run the llm-llama-cpp.py file with the path to any pdf document 

You can download gguf models from here

https://huggingface.co/models?library=gguf
